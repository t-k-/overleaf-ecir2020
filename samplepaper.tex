% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}

\usepackage{graphicx}

\usepackage{cite}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{booktabs}
\usepackage{footnote}
\usepackage{xcolor}
\makesavenoteenv{tabular}
\makesavenoteenv{table}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{marvosym}
\usepackage{hyperref}

\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
%
\title{Accelerating Substructure Similarity Search for Formula Retrieval}
\titlerunning{Accelerating Substructure Similarity Search}

\author{Wei Zhong\inst{1}\textsuperscript{,\Letter}, Shaurya Rohatgi\inst{2}, Jian Wu\inst{3}, \\ C. Lee Giles\inst{2} \and Richard Zanibbi\inst{1}\textsuperscript{,\Letter}}
\institute{
Rochester Institute of Technology \\
\email{\{wxz8033,rxzvcs\}@rit.edu}
\and Pennsylvania State University 
\and Old Dominion University
}

\authorrunning{Wei Zhong et al.}

\maketitle

\begin{abstract}
Formula retrieval systems using substructure matching are effective, but suffer from slow retrieval times caused by the complexity of structure matching.  We present a specialized inverted index and rank-safe dynamic pruning algorithm for faster substructure retrieval. Formulas are indexed from their Operator Tree (OPT) representations. Our model is evaluated using the NTCIR-12 Wikipedia Formula Browsing Task and a new formula corpus produced from Math StackExchange posts.  Our approach preserves the effectiveness of structure matching while allowing queries to be executed in real-time.

\keywords{Math information retrieval, Query processing optimization, Dynamic pruning}
\end{abstract}

\section{Introduction}
In information retrieval, a great deal of research has gone into creating efficient search engines for large corpora.
However, few have addressed substructure search in structural content, e.g., in Mathematical Information Retrieval (MIR)~\cite{survey2012} where efficient substructure similarity search is needed to to identify shared subexpressions effectively.
For example, in math formula search, to discern that $a+b$ and $b + a$ are equivalent (by commutativity), but that $ab+cd$ and $a+bcd$ are different,  applying tokenization and counting common token frequencies is insufficient. Instead, a hierarchical representation of mathematical operations is needed to identify shared substructures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]

\begin{center}
\begin{minipage}[b]{1.5in}
\begin{center}
\includegraphics[width=1.2in]{fig/tree3.eps}\\
$bc + xy + a + z$
\end{center}
\end{minipage}
\hspace*{.0in}
\begin{minipage}[b]{1.5in}
\begin{center}
\raisebox{.0in}{\includegraphics[width=0.9in]{fig/tree4.eps}}\\
$(a + bc) + xy$
\end{center}
\end{minipage}
\end{center}

\caption{Operator trees (OPTs) for two similar formulas. OPTs represent the application of operations (at internal nodes in circles) to operands (at the leaves in squares). Two common substructures are highlighted in black and gray.}
\label{intro}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the most recent math similarity search competition,\footnote{The NTCIR-12 Wikipedia Formula Browsing Task.} effective systems all take a tree-based approach by extracting query terms from tree representations.
For example, an Operator Tree (OPT) is used in Figure~\ref{intro} to represent math formulas where operands are represented by leaves and operators are located at internal nodes.
This facilitates searching substructures shared by two math expressions. For example, we can extract paths from their tree representations and find their shared subtrees by matching their common paths grouped by subtree root nodes. 
However, in order to carry structure information, it is common to see structural queries with over tens or even hundreds of path tokens which is unusual for normal fulltext search. This makes query processing costly for realistic math search tasks.

In text similarity search, query processing can be accelerated through dynamic pruning~\cite{tonellotto2018efficient}, which typically estimates score upperbounds to prune documents unlikely to be in the top K results.
%
However, effective substructure search requires additional matching or alignment among query terms, and this makes it hard to get a good score estimation  and it prevents us applying traditional dynamically pruning effectively.
In fact, reportedly few state-of-the-art MIR systems have achieved practical query run times even when given a large amount of computing resources~\cite{ntcir12, mcat_16}.
In this paper we try to address this problem by introducing a specialized inverted index and we propose a dynamic pruning method based on this  inverted index to boost formula retrieval efficiency.

\section{Related Work}
\label{relatedworks}

Recently there has been an increasing amount of research on similarity search for math formulas, with most focusing on search effectiveness~\cite{a0_2019,tangent-combine2017,mcat_16,peking2016}.
There are many emerging issues regarding effectiveness, including handling mathematical semantics, and identifying interchangeable symbols and common subexpressions. However, the efficiency of math formula search systems is often not addressed.

%
A number of MIR systems apply text search models to math retrieval, extracting sequential features from formulas and use variants of TF-IDF scoring~\cite{nist_03, mias_11, peking2014}. These approaches incorporate a bag-of-words model, and use frequency to measure formula similarity. Inevitably, they need to index different combinations of sequences or substrings to handle operator commutativity and associativity.
This index augmentation results in a non-linearly increasing index size in the number of indexed ``words''~\cite{peking2014} and thus hurts efficiency for large corpora.
%
On the other hand, recent results~\cite{ntcir12, a0_2019, tangent_2019} reveal that effective systems for formula retrieval use tree-based approaches distinct from text-based methods.
%
However, tree-based systems usually need to calculate costly graph matching or edit distance metrics~\cite{tangent-multistage2016, edit_dist2013}, which generally have non-linear time complexity.
Recently, a path-based approach~\cite{a0_2019} was developed to search substructures in formula OPTs approximately by assuming that identical formulas have the same leaf-root path set.
Although it obtains the best effectiveness for the NTCIR-12 dataset, the typically large number of query paths means that query run times are not ideal  - maximum run times can be a couple of seconds.

Dynamic pruning has been recognized as an effective way to reduce query processing times~\cite{jonassen_simon_2011, antonio2019, macdonald2011upper, tonellotto2018efficient}.
%
Dynamic pruning speeds up query processing by skipping scoring calculations or avoiding unnecessary reads for documents which are unlikely to be ranked in the top K results.
%
Pruning methods can be based on different query processing schemes:
Document-at-a-time (DAAT) requires all relevant posting lists be merged simultaneously. Term-at-a-time (TAAT) or score-at-a-time (SAAT) processes one posting list at a time for each term, requiring additional memory to store partial scores, and posting lists in this case are usually sorted by document importance (e.g, impact score~\cite{anh2006SAAT}), with promising documents placed at  the front of inverted lists.
%
Pruning strategies are \textit{rank-safe} (or \textit{safe up to rank K})~\cite{turtle_flood_1995} if they guarantee that the top K documents are ranked in the same order before and after pruning. 
%
The most well-known rank-safe pruning strategies for DAAT are MaxScore~\cite{turtle_flood_1995, strohman_turtle_2005, jonassen_simon_2011} and WAND variants~\cite{broder2003WAND, ding2011BMW}.
%\footnote{MaxScore also has TAAT version~\cite{turtle_flood_1995}.} 
%
Shan et al.~\cite{Shandongdong2012} show that MaxScore variants (e.g. BMM, LBMM) outperform other dynamic pruning strategies for long queries, and recently Mallia et al.~\cite{antonio2019} report a similar finding over a range of popular index encodings.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
\noindent\textbf{Baseline Model}\; This work is based on our previous work~\cite{a0_2019} which extracts prefixes from OPT leaf-root paths as index or query terms. The OPT is parsed from a formula in \LaTeX{}. For indexed paths, they are mapped to corresponding posting lists in an inverted index where the IDs of expressions containing the path are appended.
For query paths, the corresponding posting lists are merged and approximate matching is performed on candidates one expression at a time. The similarity score is measured from matched common subtree(s).

Because math symbols are interchangeable, paths are tokenized for better recall, e.g., variables such as $a, b, c$ are tokenized into VAR. In our tokenized path representation uppercase words denote token types, which may be for operators as well as operands (e.g., TIMES for symbols representing multiplication).
In Figure~\ref{intro}, when indexing ``bc + xy + a + z,'' its expression ID (or ExpID) will be appended to posting lists associated with tokenized prefix paths from its OPT representation, i.e.,  VAR/TIMES, VAR/ADD and VAR/TIMES/ADD.
At query processing, the shared structures highlighted in black and gray are found by matching these tokenized paths (two paths match if and only if they have the same tokenized paths, for example, ``a/+'' and ``z/+'' can be matched) and common subtree roots  are identified by grouping paths by their root nodes.
%
As a result, in addition to ExpID, the posting list entry stores the root node ID for indexed paths, in order to reconstruct matches substructures at merge time.

At query time, the similarity score is given by the size of matched common subtrees.
%
Specifically, the model chooses a number of ``widest'' matched subtree(s) (e.g., $a+bc$ is the widest matched in Figure~\ref{intro} because it has 3 common leaves and is ``wider'' than the other choices) and measure formula similarity based on the size of common subtrees.

The original Approach0 model \cite{a0_2019} matches up to three widest common subtrees and scores similarity by a weighted sum of the number of matched leaves (operands) and operators from different common subtrees $\hat{T}_q^i, \hat{T}_d^i$ of a common forest $\pi$. Operators and operand (leaf) nodes weights are controlled by parameter $\alpha$, while the weight of rooted substructures from largest to smallest are given by $\beta_i$. In the following, $|\cdot|$ indicates the size of a set:
\begin{align}
\label{eq:1}
\sum^3_{i=1} \beta_i \left( \alpha \cdot \left|\operatorname{operators}(\hat{T}_d^i)\right| + (1 - \alpha) \cdot \left|\operatorname{leaves}(\hat{T}_d^i)\right| \right)\;, \qquad (\hat{T}_q^i, \hat{T}_d^i) \in \pi
\end{align}
%
Interestingly, while multiple subtree matching boosts effectiveness, using just the widest match still outperforms other systems in terms of highly relevant results~\cite{a0_2019}.
%
The simplified similarity score based on widest common subtree between query and document OPTs $T_q, T_d$ is the widest match $w^*_{Q, D}$, formally
\begin{align}
\label{eq:2}
w^*_{Q, D} = \max_{\hat{T}_q, \hat{T}_d \in \text{CFS}(T_q,T_d)} |\operatorname{leaves}(\hat{T}_d)|
\end{align}
where
$\text{CFS}(T_q, T_d)$ are all the \textit{common formula subtrees} between $T_q$ and $T_d$.
In addition to subtree isomorphism, a \emph{formula subtree} requires leaves in a subtree to match leaves in the counterpart, in other words, subtrees are matched bottom-up from operands in OPTs.
%
In Figure~\ref{intro}, the value of $w^*_{Q, D}$ is 3, produced by the widest common subtrees shown in gray.

\vspace{0.1in}
\noindent\textbf{Dynamic Pruning}\; In dynamic pruning, the top K scored hits are kept throughout the querying process, with the lowest score in the top K at a given point defining the threshold $\theta$. Since at most K candidates will be returned, dynamic pruning strategies work by estimating score upperbounds before knowing the precise score of a hit so that candidate hits with a score upperbound less or equal to $\theta$ can be pruned safely, because they will not appear in the final top K results.
Moreover, if a subset of posting lists alone cannot produce a top K result from their upperbounds, they are called a \textit{non-requirement set}, the opposite being the \textit{requirement set}.
%
Posting lists in the non-requirement with IDs less than the currently evaluating IDs in the requirement set can be skipped safely, because posting lists in the non-requirement set alone will not produce a top K candidate.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{strategy}
In this paper, we apply dynamic pruning to structural search.
As structure search has more query terms in general, we focus on a MaxScore-like strategy suggested by~\cite{Shandongdong2012, antonio2019}, since they do not need to sort query terms at merge iterations (which is expensive for long queries).
%
Our approach is different from the original MaxScore, as upperbound scores are also calculated from the query tree representation.
We also use the simplified scoring equation~(\ref{eq:2})
where a subset of query terms in the widest matched common subtrees $\hat{T}_q, \hat{T}_d$ contribute to the score.
In contrast, typical TF-IDF scoring has all hit terms contribute to the rank score.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
\begin{center}
\includegraphics[width=1.90in]{fig/bipartile.eps}
\caption{Bipartite graph of hit path set for formulas in Figure~\ref{intro} (original leaf symbol is used here to help identify paths). Edges are established if paths from the two sides are the same after tokenization. Edges with shared end points (i.e., same root-end nodes) in original OPTs have the same color (black or gray). }
\label{bipart}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When we merge posting lists, a set of query paths match paths from a document expression one at a time, each time a \textit{hit path set} for matched query and candidate paths are examined.
%
Define $\mathcal{P}(T)$ to be all paths extracted from OPT $T$, i.e.,
$\mathcal{P}(T) = \{p: p \in \operatorname{leafroot\_paths}(T^n), n \in T\}$ where $T^n$ is the entire subtree of $T$ rooted at $n$ with all its descendants.
%
We model the hit path set by a bipartite graph 
%%
$G(Q, D, E)$ where $Q = \{q: q \in \mathcal{P}(T_q)\}, D = \{d: d \in \mathcal{P}(T_d)\}$ are query and document path sets, and edges are ordered pairs $E = \{(q, d): \operatorname{tokenized}(q) =  \operatorname{tokenized}(d), q \in Q, d \in D\}$ representing a potential match between a query path to a document path.
Since an edge is established only for paths with the same token sequence, we can partition the graph into disconnected smaller bipartite graphs $G_t = G(Q_t, D_t, E_t)$, each identified by tokenized query path $t$:
$$
\begin{aligned}
Q_t &= \{ q: q \in Q, \operatorname{tokenized}(q) = t\} \\
D_t &= \{d: d \in D, \operatorname{tokenized}(d) = t \} \\
E_t &= \{(q, d): (q, d) \in E, \operatorname{tokenized}(q) = \operatorname{tokenized}(d) \}
\end{aligned}
$$

Figure~\ref{bipart} shows the hit path set of the example in Figure~\ref{intro}, this example can be partitioned into independent subgraphs associated with tokenized paths VAR/TIMES/ADD, VAR/TIMES and VAR/ADD.
Each partition is actually a complete bipartite graph (fully connected) because for any edge between $Q_t$ and $D_t$, it is in edge set $E_t$.
And for each complete bipartite graph $G(Q_t, D_t, E_t)$, we can obtain their maximum matching sizes from $\min(|Q_t|, |D_t|)$ easily.

On the other hand, to calculate score $w^*_{Q, D}$, we need to find a pair of query and document nodes at which the widest common subtree $\hat{T}^*_q, \hat{T}^*_d$ are rooted (see equation~\ref{eq:2}), so we also define the matching candidate relations filtered by nodes.
Let $G^{(m, n)} = G(Q^{(m)}, D^{(n)}, E^{(m, n)})$ be the subgraph matching between query subtree rooted at $n$ and document subtree rooted at $m$  where
$$
\begin{aligned}
Q^{(m)} &= \{ q: q \in Q, \operatorname{root\_end}(q) = m\} \\
D^{(n)} &= \{d: d \in D, \operatorname{root\_end}(d) = n\} \\
E^{(m, n)} &= \{(q, d): (q, d) \in E, \operatorname{root\_end}(q) = m, \operatorname{root\_end}(d) = n\} 
\end{aligned}
$$
Then, similarity score $w^*_{Q, D}$ can be calculated from node pairs and their partitions.
Specifically, define \textit{token paths} of tree $T$ rooted at $n$ as set $\mathfrak{T}(n) = \{t: t = \operatorname{tokenized}(p), p \in \operatorname{leafroot\_paths}(T^n) \}$,
\begin{align}
w^*_{Q, D} &= \max_{m \in T_q, n \in T_d} \; \nu(G^{(m, n)}) \\
           &= \max_{m \in T_q, n \in T_d} \sum_{t \in \mathfrak{T}(m)} \nu(G^{(m, n)}_t) \\
		   \label{eq:5}
           &= \max_{m \in T_q, n \in T_d} \sum_{t \in \mathfrak{T}(m)} \min(|Q^{(m)}_t|, |D^{(n)}_t|)
\end{align}
where $\nu(G)$ is the maximum matching size of bipartite graph $G$.

Denote $w_{m, t} = |Q^{(m)}_t|$, we call $w_{m, t} \ge \min(|Q^{(m)}_t|, |D^{(n)}_t|)$ as our (pre-computed) partial score upperbound.
It is analogous to text search where each posting list has a partial score upperbound, the TF-IDF score upperbound is merely their sum. In our case, the sum for partial score upperbounds is only for one node or subtree.

In the following we propose three strategies to compute $w^*_{Q, D}$ upperbound from partial score upperbounds and assign non-requirement set.

\vspace{0.1in}
\noindent \textbf{Max reference (MaxRef) strategy}\;
In MaxScore~\cite{turtle_flood_1995, strohman_turtle_2005}, each posting list has a partial score upperbound, however, our scoring function implies each posting list can be involved with multiple partial score upperbounds.
One way to select the non-requirement set in our case is using an upperbound score $MaxRef_t$ (for each posting list $t$) which is the maximum partial score from the query nodes by which this posting list gets ``referenced'', and if a set of posting lists alone has a sum of MaxRef scores less or equal to $\theta$, they can be safely put into the non-requirement set.

The rank safety can be justified, since each posting list corresponds to a unique tokenized path $t$, and $\text{MaxRef}_t = \max_m w_{m, t}$. Then for $m \in T_q, n \in T_d$,
\begin{align}
\label{eq:6}
\sum_t \min(|Q^{(m)}_t|, |D^{(n)}_t|) \le \sum_t w_{m,t} \le \sum_t \text{MaxRef}_t
\end{align}
then the selection of non-requirement set (named \textbf{Skip} set for short) such that 
$\sum_{t \in \textbf{Skip}} \text{MaxRef}_t \le \theta$
follows $w^*_{Q, D} \le \theta$ for all non-requirement set posting lists.

\vspace{0.1in}
\noindent \textbf{Greedy binary programming (GBP) strategies}\;
%
Inequality~(\ref{eq:6}) is relaxed twice, so it spurs the motivation to get tighter upperbound value by maximizing the number of posting lists in the non-requirement set, so that more posting lists are likely to be skipped.
Define partial upperbound matrix $\mathbf{W} = \{ w_{i, j} \}_{|T_q| \times |\mathfrak{T}|} $ where $\mathfrak{T} = \{ \mathfrak{T}(m), m \in T_q \}$ are all the token paths from query OPT ($\mathfrak{T}$ is essentially the same as tokenized $\mathcal{P}(T_q)$), and a binary variable $\boldsymbol{x}_{|\mathfrak{T}| \times 1}$ indicating which corresponding posting lists are placed in the non-requirement set.
One heuristic objective is to maximize the number of posting lists in the non-requirement set (GBP-NUM):
\begin{align}
\label{eq:obj}
\text{maximize} && \mathbf{1} \cdot \boldsymbol{x} \\
\label{eq:7}
s.t.            && \mathbf{W} \boldsymbol{x} \le \theta
\end{align}
However, maximizing the number of posting lists in the non-requirement set does not necessarily cause more items to be skipped, because the posting lists can be very short.
Instead, we can maximize the total length of posting lists in the non-requirement set. In this case, the vector of ones in (\ref{eq:obj}) is replaced with posting list length vector
$\mathbf{L} = \begin{bmatrix} L_1, L_2, \ldots L_{|\mathfrak{T}|}\end{bmatrix}$, where $L_i$ is the length of posting list $i$.
We call this strategy GBP-LEN.
%
The two GBP strategies are rank-safe % because $\forall\; m \in T_q, n \in T_d$,
% \begin{align}
% \label{eq:9}
% \sum_t \min(|Q^{(m)}_t|, |D^{(n)}_t|) \le \sum_t w_{m,t}
% \end{align}
since constraints in inequality~(\ref{eq:7}) requires $\sum_{t \in \textbf{Skip}} w_{m,t} \le \theta$.

Both strategies require solving binary programming problems, which are known to be NP-complete and thus too intensive for long queries.
Instead, we greedily follow one branch of the binary programming sub-problems to obtain a feasible (but not optimal) solution in $O(|T_q| |\mathfrak{T}|^2)$.
%Greedy solutions also meet these constraints, thus it is still rank-safe.

%\input{gbp.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}
%\subsection{Indices structure}

Figure~\ref{figillu} illustrates formula query processing using a modified inverted index for dynamic pruning.
For each internal node $m$ of the query OPT, we store the number of leaves of $m$  as $w_m = |Q^{(m)}|$. 
Each query node points to tokenized path entries in a dictionary,
where each reference is associated with $w_{m,t} = |Q^{(m)}_t|$ identified by tokenized path $t$ (denoted as $m/w_m$ of $t$).
%
In Figure~\ref{figillu}, node $q1$ from the query has 6 leaves, which is also the upperbound number of path matches for $q1$, i.e, $|Q^{(1)}|$.
Since $q1$ consists of 2 tokenized leaf-root paths VAR/TIMES/ADD and VAR/ADD,
$q1$ is linked to two posting lists, each associated with a partial upperbound (5 and 1).

Each posting list maps to a token path $t \in \mathfrak{T}$ with a dynamic counter for the number of query nodes referring to it (initially $|Q_t|$).
Query nodes are pruned by our algorithm when its subtree width is no longer greater than the current threshold, because the corresponding subexpression cannot be in the top-K results. In this case
the reference counter decreases. A posting list is removed if its reference counter is less than one.

Each posting list entry identified by an ExpID stores $n$ and $w_{n,t} = |D^{(n)}_t|$ values of subtree token path $t$ rooted at $n$ (denoted as $n/w_n$ of $t$).
%
As an example, in Figure~\ref{figillu}, the hit OPT (of ExpID 12) has 5 paths tokenized as $t =$ VAR/TIMES/ADD, 2 rooted at $d4$ and 3 rooted at $d1$. The information ($d1/3, d4/2$) is stored with corresponding posing list $t$.
In our implementation, each posting list is traversed by an iterator ($iters[t]$), and its entries are read by $iters[t].read()$ from the current position accessed by iterator.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
\begin{center}
\includegraphics[width=\textwidth]{fig/drawing.eps}
\caption{Indices for formula search with dynamic pruning. For MaxRef strategy, the top posting list is the only one in the requirement set. The bottom two posting lists are advanced by skipping to next candidate ExpID.}
\label{figillu}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[!b]
\scriptsize
\caption{Formula searching algorithm with pruning}
\label{alg1}
\begin{algorithmic}[1]
\Function{qryNodeMatch}{iters, $m$, candidate, widest, $\theta$}
	\State nodeMatch[ ] := $\vec{0}$; $\ell$ := $|\operatorname{leaves}(m)|$
	\Comment{$\ell$ is the leftover estimated upperbound.}
 	\For {each $m/w_m$ of tokenized path $t$ rooted at $m$}
 	    \State Let $i$ be the iterator index associated with $t$
		\If {iters[$i$].expID $<$ candidate}
			\State iters[$i$].skipTo(candidate)
		\EndIf
		\If {iters[$i$].expID $=$ candidate }
		\For {each $n/w_n \;\text{of}\; t$ from iters[$i$].read() }
 			\State nodeMatch[$n$] := nodeMatch[$n$] + min($w_m$, $w_n$)
 		\EndFor
		\EndIf
		\State $\ell$ := $\ell - w_m$;
		\State estimate := max(nodeMatch) + $\ell$ 
		\Comment{Update estimation.}
		\If {estimate $\le$ widest \textbf{or} $u$(estimate) $\le \theta$}
			\State \Return 0
		\EndIf
 	\EndFor
 	\State \Return max(nodeMatch)
\EndFunction
~\\
\Function{FormulaSearch}{iters, strategy}
	\State $\theta$ := $0$; reqs := \Call{RequirementSet}{$\theta$, strategy}
	\State heap := data structure to hold top K results
	\While {true}
		\State candidate := minimal ID in current expIDs of reqs
		\If {candidate equals MaxID}
            \Comment{Search terminated, return results.}
			\State \Return top K results
		\EndIf
		\State Let $G(Q, D, E)$ be the hit path set bipartite graph.
		\State widest := 0; hitNodes := $\{\operatorname{root\_end}(q): (q, d) \in E \}$
		\For {$m$ in hitNodes}
            \Comment{Calculate maximum match for each hit query node.}
			\If {$|\operatorname{leaves}(m)|$ $\le$ widest}
				\State \textbf{continue}
			\EndIf
			\State maxMatch := \Call{qryNodeMatch}{iters, $m$, candidate, widest, $\theta$}
			\If {maxMatch $>$ widest} widest := maxMatch \Comment{Find the widest width.} \EndIf
		\EndFor
		\If {widest $> 0$}
		    \State score := calculate final score (including symbol similarity).
			\Comment{See [23].}
			\If {heap is not full {\bf or} score $> \theta$}
				\State Push candidate or replace the lowest scored hit in heap.
				\If {heap is full}
				\Comment{Update current threshold.}
					\State $\theta$ := minimal score in current top K results
					\State Drop small query nodes and unreferenced iterators.
					\State reqs := \Call{RequirementSet}{$\theta$, strategy}
					\Comment{Update requirement set.}
				\EndIf
			\EndIf
		\EndIf

		\For {iters[$i$] in reqs}
            \Comment{Advance posting list iterators.}
			\If {iters[$i$].expID = candidate}
				iters[$i$].next()
			\EndIf
		\EndFor
	\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Query processing is described in Algorithm~\ref{alg1}.
\Call{RequirementSet}{} returns selected iterators of the requirement set.
%(complement of the non-requirement set).
Assignment according to different pruning strategies is described in Section~\ref{strategy}.
%
In the MaxRef strategy, we sort posting lists by descending $\text{MaxRef}$ values, and take as many posting lists as possible into non-requirement set from the lowest $\text{MaxRef}$ value.
At merging, a \textit{candidate} ID is identified by assigning the minimal ExpID of current posting list iterators in the requirement set.
Requirement set iterators are advanced by one using the \textit{next()} function, while iterators in the non-requirement set are advanced directly to the ID equal to or greater than the current candidate by the \textit{skipTo()} function. In Figure~\ref{figillu} for example, the posting list corresponding to VAR/TIMES/ADD is in the requirement set under the MaxRef strategy, while the other two are not: Document expression 13 and 15 will be skipped if the next candidate is 90.
%
For ease of testing termination, we append a special ExpID \emph{MaxID} at the end of each posting list, which is larger than any ExpID in the collection.

At each iteration, a set of \textit{hitNodes} is inferred containing query nodes associated with posting lists whose current ExpIDs are candidate ID.
\Call{qryNodeMatch}{} calculates matches for hit nodes according to equation~\ref{eq:5}, pruning nodes whose maximum matching size is smaller than than previously examined nodes.
%matched size of an already calculated node.
%
Given query hit node $q1$ in Figure~\ref{figillu}, function \Call{qryNodeMatch}{} returns
$$\max_{n\in T_d}\;\nu(G^{(1, n)}) = \max(\min(5, 2) + \min(1, 2),\; \min(5, 3)) = 3$$
Then the algorithm selects the best matched query node and its matched width (i.e., \emph{widest} in Algorithm~\ref{alg1}) is our structural similarity $w^*_{Q, D}$.

After obtaining $w^*_{Q, D}$, we compute a metric for the similarity of symbols (e.g., to differentiate $E=mc^2$ and $y=ax^2$) and penalize larger formulas, to produce a final \emph{overall similarity score}~\cite{a0_2019} for ranking.
Because of this additional layer, we need to relax our upperbound further.
According to the overall scoring function in \cite{a0_2019}, our relaxing function $u$ can be defined by assuming perfect symbol similarity score in overall scoring function, specifically
\begin{equation}
u(w) = \frac{w}{|\text{leaves}(T_q)| + w} \left[ (1 - \eta) + \eta \, \frac 1 {\log (1 + n_d)} \right]
\end{equation}
where in our setting,  parameters $\eta = 0.05, n_d = 1$.
%
Whenever threshold $\theta$ is updated, we will examine all the query nodes, if a query node $m$ has an upperbound less or equal to the threshold, i.e., $u(m) \le \theta$, then the corresponding subtree of this node is too ``small'' to make it into top K results. As a result, some of the posting lists (or iterators) may also be dropped due to zero reference.

\section{Evaluation}
We first evaluate our system\footnote{Source code: \url{https://github.com/approach0/search-engine/tree/ecir2020}} on the NTCIR-12 Wikipedia Formula Browsing Task~\cite{ntcir12} (NTCIR-12 for short), which is the most current benchmark for formula-only retrieval.
The dataset contains over 590,000 math expressions taken from English Wikipedia.
%
Since work in formula retrieval is relatively new, there are only 40 queries in NTCIR-12 that can be compared with other published systems. However, these queries are well designed to cover a variety of math expressions in different complexity.
%and it is still meaningful to compare for efficiency evaluation. 
There are 20 queries containing wildcards (using wildcard specifier \textbf{\textbackslash{}qvar} to match arbitrary subexpression or symbols, e.g., query ``\textbackslash qvar\{a\}$^2$ + \textbackslash qvar\{b\}$^3$'' can match ``$x^2 + (y + 1)^3$'').

We add support for wildcards by simply treating internal nodes (representing a rooted subexpression) of formulas as additional ``leaves'' (by ignoring their descendants), and the wildcard specifiers in a query are treated as normal leaves to match those indexed wildcard paths.
Since the corpus of NTCIR-12 is not large enough to show the full impact of pruning, we also evaluate query run times on a corpus containing over 1 million math related documents/threads from Math StackExchange (MSE) Q\&A website\footnote{MSE corpus: \url{https://www.cs.rit.edu/~dprl/data/mse-corpus.tar.gz}} and we run the same query set from NTCIR-12.
%
Run times are shown for the posting list merging stage (e.g., time for parsing the query into OPT is excluded) and unless specified, posting lists are compressed and cached into memory.
%
Each system had five independent runs, and we report results from overall distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[]
\small
	\centering
    \caption{Query merge time performance (in milliseconds) for different strategies.}
	%, returning different maximum number of results (K value), on two dataset Wikipedia (Wiki) and Math StackExchange (MSE).}
	\resizebox{\textwidth}{33mm}{
	\begin{tabular}{lrl|ccccc|ccccc}
	\toprule
	\multicolumn{3}{c|}{\bf{Runs}} &
	\multicolumn{5}{c|}{\bf Non-wildcards} & \multicolumn{5}{c}{\bf Wildcards} \\
	& \bf K & \bf Strategy &
	\bf $\mu$ & \bf $\sigma$ & \bf median & \bf min & \bf max & 
	\bf $\mu$ & \bf $\sigma$ & \bf median & \bf min & \bf max \\
	\hline
	% wiki&base&100
	% & avg & std & med & min & max & avg & std & med & min & max \\
	%%%%%%%%% INSERT BELOW %%%%%%%%%%
	\multirow{10}{*}{ \rotatebox[origin=c]{90}{Wiki Dataset}   }

& 100 & Baseline
& 540.12& 569.44& 360.50& 7.00& 2238.00& 426.73& 383.47& 225.50& 8.00& 1338.00\\

	\cline{2-13}

& 100 & MaxRef
& 90.29& 74.14& 79.00& 3.00& 312.00& 145.50& 121.19& 136.00&\bf 7.00& 573.00\\
&  & GBP-NUM
& 84.90& 80.44& 52.50& 3.00& 321.00& 138.82& 102.55& 135.00& 9.00& 428.00\\
&  & GBP-LEN
& \bf 67.49& \bf 61.40& \bf 45.00& \bf 2.00& \bf 218.00& \bf 125.27&\bf 97.28&\bf 103.50& 9.00& \bf 404.00\\

	\cline{2-13}

& 200 & MaxRef
& 107.71& \bf 82.64& 102.00&\bf 5.00&\bf 322.00& 160.10& 121.40& 149.00& 9.00& 583.00\\
&  & GBP-NUM
& 105.34& 99.51& 71.50&\bf 5.00& 357.00& 155.52& 110.61& 153.00& \bf 8.00& 479.00\\
&  & GBP-LEN
& \bf 89.63& 83.20& \bf 62.00& \bf 5.00& 330.00& \bf 142.78& \bf 103.11& \bf 143.50& 9.00& \bf 446.00\\

	\cline{2-13}

& 1000 & MaxRef
& 154.51& \bf 93.75& 157.50& \bf 6.00& \bf 361.00& 211.86& 140.01& 186.00& 10.00& 662.00\\ 
&  & GBP-NUM
& 159.80& 143.70& 120.50& \bf 6.00& 626.00& 208.91& 136.42& 178.50& 10.00& 591.00\\ 
&  & GBP-LEN
& \bf 144.25& 126.95& \bf 105.00& \bf 6.00& 622.00& \bf 195.70& \bf 122.25& \bf 176.00& \bf 9.00& \bf 536.00\\ 

	\midrule
	\midrule
	\multirow{10}{*}{ \rotatebox[origin=c]{90}{MSE Dataset}   }

& 100 & Baseline
& 15134.10& 15186.78& 11161.00& 157.00& 55499.00& 13450.57& 12554.19& 7075.50& 304.00& 47513.00\\

	\cline{2-13}

& 100 & MaxRef
& 1083.23& 1274.23& 745.50& 28.00& 5922.00& 3188.66& 2458.91& 2925.00& 85.00& 10412.00\\
&  & GBP-NUM
& 1202.24& 1240.21& 815.00& 37.00& 4987.00& 2943.79& 2025.96& 2987.00&\bf 84.00& 8775.00\\
&  & GBP-LEN
& \bf 562.83& \bf 635.26& \bf 382.50&\bf 24.00&\bf 2313.00&\bf 2257.95&\bf 1491.59&\bf 2346.50& 86.00& \bf 4494.00\\


	\cline{2-13}

& 200 & MaxRef
& 1261.21& 1368.93& 1012.50& 30.00& 6439.00& 3416.77& 2753.09& 3032.50& 160.00& 12412.00\\
&  & GBP-NUM
& 1378.19& 1398.08& 998.50& 39.00& 5863.00& 3174.93& 2283.05& 3125.00& \bf 159.00& 10099.00\\
&  & GBP-LEN
& \bf 697.32&\bf 739.11& \bf 478.00& \bf 27.00& \bf 2925.00& \bf 2504.90& \bf 1683.16& \bf 2382.50& \bf 159.00& \bf 6049.00\\

	\cline{2-13}

& 1000 & MaxRef
& 2030.05& 1746.17& 1796.50& 53.00& 7816.00& 4123.26& 3510.01& 3473.00& 287.00& 16981.00\\ 
&  & GBP-NUM
& 1952.52& 1746.05& 1530.50& 60.00& 7197.00& 3786.89& 2744.99& 3493.50& \bf 281.00& 11323.00\\ 
&  & GBP-LEN
& \bf 1217.16& \bf 1083.53&\bf 764.50&\bf 47.00&\bf 3756.00&\bf 3304.69&\bf 2403.09&\bf 2812.00& 285.00& \bf 9895.00\\ 

	%%%%%%%%% INSERT ABOVE %%%%%%%%%%
	\bottomrule

	\end{tabular}
	}
\label{majortab}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table~\ref{majortab} reports run time statistics.
Non-pruning (exhaustive search) baselines with K = 100 are also compared here.
Almost consistently, GBP-LEN strategy achieves the best efficiency with smaller variance.
This is expected since GBP-LEN models the skipping possibility better than GBP-NUM. Although GBP-NUM gives a tighter theoretic upperbound than MaxRef, it only maximizes the number of posting lists in the non-requirement set and may lead to bad performance when these posting lists are short.

There are a few times the best minimal run times are from other strategies, for those with meaningful gaps, i.e., in Wiki dataset of non-wildcard queries when K = 1000, MaxRef outperforms in standard deviation and maximum run time to a notable margin; however, it likely results from a small threshold due to large K, so that the efficiency on the small sized NTCIR dataset is less affected by pruning (small $\theta$ means less pruning potential) compared to the time complexity added from assigning to the requirement set. The latter is more dominant in GBP runs. In wildcard queries, however, many expressions can match the query thus the threshold value is expected to be larger than that in the non-wildcard case.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
\begin{center}

%\hspace*{-3.6cm} 
%\resizebox{\textwidth}{!}{

	%\begin{minipage}[b]{1.5in}
	%\resizebox{\textwidth}{0.29in}{
	\begin{tabular}{l|cc|cc|cc}
	\toprule
	\multirow{2}{*}{\bf{System}}
	& \multicolumn{2}{c|}{\bf Non-Wildcard}
	& \multicolumn{2}{c|}{\bf Wildcard}
	& \multicolumn{2}{c}{\bf All queries} \\
	& Full& Partial
	& Full& Partial
	& Full& Partial \\
 	\toprule
 	MCAT                &     .5678 &     .5698 & \bf .4725 &     .5015 &     .5202 &     .5356 \\
{Tangent-S} &     .6361 &     .5872 &     .4699 & \bf .5368 & \bf .5530 & \bf .5620 \\
{base-best} & \bf .6726 & \bf .5950 &     -     &      -    &      -    &       -   \\
{base-opd-only} & .6586 &   .5153 &     -     &      -    &      -    &       -   \\
 	Ours (pruning)&     .6586 &     .5173 &     .3678 &     .3973 &     .5132 &     .4573 \\
 	Ours (exhaustive)&     .6586 &     .5173 &     .3678 &     .3973 &     .5132 &     .4573 \\
 	\bottomrule
	\end{tabular}
%	}
%	\end{minipage}

\end{center}
\caption{Bpref~\cite{buckley2004retrieval} scores. Bpref chosen because we did not participate in NTCIR-12 and did not contribute to the pooling.
}
\label{bpref}
\end{figure}

\begin{figure}[!t]
\begin{center}
\hspace*{-0.1in}

\includegraphics[width=\textwidth]{fig/boxplot.eps}
\vspace{-0.25in}

\caption{Average run times on the same machine (Environment: Intel Core i5 @ 3.60GHz per core, 16 GB memory and SSD drive) for NTCIR-12 Wiki Formula Browsing Task.}
\label{boxplot}
\end{center}

\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Secondly, we have compared our system effectiveness (Figure \ref{bpref}) and efficiency (Figure \ref{boxplot}) with Tangent-S~\cite{tangent-combine2017}, MCAT~\cite{mcat_16} and our baseline system without pruning~\cite{a0_2019},
which are all structure-based formula search engines  that have obtained the best published Bpref scores on NTCIR-12 dataset.
In addition, ICST system~\cite{peking2016} also obtains effective results for math and text mixed task, but they do training on previous Wiki dataset and their system is currently not available.

All systems are evaluated in a single thread for top-1000 results.
%
We use our best performance strategy, i.e., GBP-LEN, having an on-disk version with posting lists uncompressed and always read from disk, and an in-memory version with compression.
%
For the baseline system, only 20 non-wildcard queries are reported because it does not support wildcards. 
We compare the baseline best performed run (base-best) which uses costly multiple tree matching as well as its specialized version (base-opd-only) which considers only the largest matched tree width (see equation~\ref{eq:2}).
%
Tangent-S has a few outliers as a result of its costly alignment algorithm to rerank structure and find the Maximum Subtree Similarity~\cite{tangent-multistage2016}, its non-linear complexity makes it expensive for some long queries (especially in wildcard case).
%
And MCAT reportedly has a median query execution time around 25 seconds, using a server machine and multi-threading~\cite{mcat_16}.
So we remove Tangent-S outliers and MCAT from runtime boxplot.
For space, we only include the faster base-opd-only baseline in Figure~\ref{boxplot}.

We outperform Tangent-S in efficiency even if we exclude their outlier queries, with higher Bpref in non-wildcard fully relevant results. Our efficiency is also better than the baseline system, even if the latter only considers less complex non-wildcard queries.
%
However, our overall effectiveness is skewed by bad performance of wildcard queries because a much more expensive phase is introduced to boost accuracy by other systems to handle inherently difficult ``structrual wildcards.''

Our pruning strategies are rank-safe (pruning and exhaustive version shows the same Bpref scores) but there is a minor Bpref difference between ours and baseline (base-opd-only) due to parser changes we have applied to support wildcards (e.g., handle single left brace array as seen in a wildcard query) and they happen to slightly improve accuracy in partially relevant cases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[]
% \begin{center}
% \includegraphics[height=1.6in]{fig/theta-factors.eps}
% \caption{Effect of initial thresholds on performance. The left axis shows run times in msecs (bars), the right axis shows Bpref scores (lines).}
% \label{thresholds}
% \end{center}
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
We have presented rank-safe dynamic pruning strategies that produce an upperbound estimation of structural similarity in order to speedup formula search using subtree matching.
Our dynamic pruning strategies and specialized inverted index are different from traditional linear text search pruning methods and they further associate query structure representation with posting lists.
Our results show we can obtain substantial improvement in efficiency over the baseline model, while still generating highly relevant non-wildcard search results.
Our approach can process a diverse set of structural queries in real time.
% ---- Bibliography ----
\bibliographystyle{splncs04}
\bibliography{rpa}
\end{document}
